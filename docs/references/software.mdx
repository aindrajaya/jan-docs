---
sidebar_position: 2
---

# More Software Info

## About `ctx_len` value

 - The typical `ctx_len` of `2048` is standard in language models such as `GPT-3.5`, offering sufficient context for generating responses.
 - The **maximum** `ctx_len` is `4096`, enabling the model to consider a broader context.
 - The **minimum** `ctx_len` is `1`, enabling the model to concentrate solely on the input prompt..

## About `ngl` value

 - Reducing `ngl` from `400` to `100` doesn't cause errors. 
 - Essentially, at `ngl = 100`, all model layers load onto the GPU, typically containing 32-64 layers. Therefore, the behavior of `ngl` at `400` is similar to that at `100`.

## About `cont_batching`

- Utilize `cont_batching` to boost throughput and efficiency by consolidating multiple requests for the same model execution. This is particularly advantageous for large language model inference, resulting in quicker response times and higher throughput.

### Use cases for `cont_batching`

 - `cont_batching` is useful in scenarios like chatbots, where multiple requests for the same model execution are common. It allows these applications to handle multiple user queries together, cutting down on requests and boosting performance.
 - `cont_batching` is also valuable when model execution takes time, like with large language models. Merging multiple requests into one batch improves efficiency, resulting in faster responses and a better user experience.

 :::warning

`cont_batching` should be used cautiously as it may not fit all scenarios. For instance, if model execution is brief or requests vary greatly, it might not enhance performance. Also, it may not work with all models or inference backends.

 :::