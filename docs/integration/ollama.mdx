---
sidebar_position: 9
---

# Ollama

## Overview

This tutorial illustrates how to integrate with Ollama using Jan.

## How to Integrate Ollama with Jan

### Step 1: Start the Ollama Server

1. Choose your model from the [Ollama library](https://ollama.com/library).
2. Execute your model with this command:

```sh
ollama run <model-name>
```

3. Referencing the [Ollama documentation on OpenAI compatibility](https://github.com/ollama/ollama/blob/main/docs/openai.md), utilize the `http://localhost:11434/v1/chat/completions` endpoint to interact with the Ollama server. 
4. Update the `openai.json` file in `~/jan/engines` with the complete URL of the Ollama server.

```json title="~/jan/engines/openai.json"
{
  "full_url": "http://localhost:11434/v1/chat/completions"
}
```

### Step 2: Model Configuration

1. Navigate to the `~/jan/models` folder.
2. Create a folder named `(ollam-modelname)`, for example, `lmstudio-phi-2`.
3. Create a `model.json` file inside the folder including the following configurations:
  - Set the `id` property to the model name as Ollama model name.
  - Set the `format` property to `api`.
  - Set the `engine` property to `openai`.
  - Set the `state` property to `ready`.

```json title="~/jan/models/llama2/model.json"
{
  "sources": [
    {
      "filename": "llama2",
      "url": "https://ollama.com/library/llama2"
    }
  ],
  "id": "llama2",
  "object": "model",
  "name": "Ollama - Llama2",
  "version": "1.0",
  "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
  "format": "api",
  "settings": {},
  "parameters": {},
  "metadata": {
    "author": "Meta",
    "tags": ["General", "Big Context Length"]
  },
  "engine": "openai"
}
```
#### Regarding `model.json`

- In `settings`, two crucial values are:
  - `ctx_len`: Defined based on the model's context size.
  - `prompt_template`: Defined based on the model's trained template (e.g., ChatML, Alpaca).
  - To set up the `prompt_template`:
    1. Visit [Hugging Face](https://huggingface.co/), an open-source machine learning platform.
    2. Find the current model that you're using (e.g., [Gemma 7b it](https://huggingface.co/google/gemma-7b-it)).
    3. Review the text and identify the template.
- In `parameters`, consider the following options. The fields in `parameters` are typically general and can be the same across models. An example is provided below:

```json
"parameters":{
  "temperature": 0.7,
  "top_p": 0.95,
  "stream": true,
  "max_tokens": 4096,
  "frequency_penalty": 0,
  "presence_penalty": 0
}
```

### Step 3: Start the Model
