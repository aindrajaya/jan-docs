---
sidebar_position: 8
---

# Jan Integration with LM Studio

## Overview

This tutorial illustrates how to integrate with LM STudio using Jan.

[LM Studio](https://lmstudio.ai/) enables you to explore, download, and run local Large Language Models (LLMs). This guide demonstrates two methods to integrate and utilize your existing LM Studio models with Jan. The first method involves integrating the LM Studio server with Jan UI, while the second method involves migrating your downloaded model from LM Studio to Jan. We'll illustrate using the [Phi 2 - GGUF](https://huggingface.co/TheBloke/phi-2-GGUF) model from Hugging Face as an example.

## Integrating LM Studio with Jan

### Step 1: Start the LM Studio Server

1. Go to the Local Inference Server within the LM Studio app.
2. Choose the desired model.
3. Initiate the server after adjusting the server port and options as needed.

Update the `openai.json` file in the `~/jan/engines` folder to include the complete URL of the LM Studio server.

```json title="~/jan/engines/openai.json"
{
  "full_url": "http://localhost:<port>/v1/chat/completions"
}
```

:::note

Replace `(port)` with the port number you set in the LM Studio server. The default port is 1234.

:::

### Step 2: Modify a Model JSON

1. Go to the `~/jan/models` directory.
2. Make a folder named `lmstudio-(modelname)` (i.e., `lmstudio-phi-2`). 
3. Create a `model.json` file with these settings:
  - Set `format` to `api`.
  - Set `engine` to `openai`.
  - Set `state` to `ready`.
  
```json title="~/jan/models/mistral-tiny/model.json"
{
  "sources": [
    {
      "filename": "phi-2-GGUF",
      "url": "https://huggingface.co/TheBloke/phi-2-GGUF"
    }
  ],
  "id": "lmstudio-phi-2",
  "object": "model",
  "name": "LM Studio - Phi 2 - GGUF",
  "version": "1.0",
  "description": "TheBloke/phi-2-GGUF",
  "format": "api",
  "settings": {},
  "parameters": {},
  "metadata": {
    "author": "Microsoft",
    "tags": ["General", "Big Context Length"]
  },
  "engine": "openai"
}
```

### Step 3: Start the Model

Restart Jan and go to the Hub. Find your model and select the Use button.

## Migrating Your Downloaded Model from LM Studio to Jan (version 0.4.6 and older)

### Step 1: Migrate Your Downloaded Model

1. Go to `My Models` in LM Studio and open the model folder.
2. Copy the model folder to `~/jan/models`.
3. Make sure the folder name matches the model name in the `.gguf` filename. Rename the folder if needed. For instance, we changed `TheBloke` to `phi-2.Q4_K_S`.

### Step 2: Start the Model

1. Restart Jan and go to the **Hub** where the model will be automatically detected and displayed.
2. Find your model and click **Use** to try it out.

## Directing to the Downloaded Model of LM Studio from Jan (version 0.4.7+)

From version 0.4.7 onwards, Jan supports importing models with an absolute filepath, allowing direct use from the LM Studio folder.

### Step 1: Reveal the Model Absolute Path

1. Go to `My Models` in the LM Studio app and access the folder containing your model.
2. Obtain the absolute path of your model.

### Step 2: Modify a Model JSON


1. Go to the `~/jan/models` directory.
2. Create a folder named `(modelname)` (e.g., `phi-2.Q4_K_S`).
3. Inside the folder, create a `model.json` file with these settings:
   - Set the `id` property to match the folder name you created.
   - Use the direct binary download link ending in `.gguf` as the `url`. You can now use the absolute filepath of the model file. For instance, `/Users/<username>/.cache/lm-studio/models/TheBloke/phi-2-GGUF/phi-2.Q4_K_S.gguf`.
   - Set the `engine` property to `nitro`.

```json title="~/jan/models/mistral-tiny/model.json"
{
  "object": "model",
  "version": 1,
  "format": "gguf",
  "sources": [
    {
      "filename": "phi-2.Q4_K_S.gguf",
      "url": "<absolute-path-of-model-file>"
    }
  ],
  "id": "phi-2.Q4_K_S",
  "name": "phi-2.Q4_K_S",
  "created": 1708308111506,
  "description": "phi-2.Q4_K_S - user self import model",
  "settings": {
    "ctx_len": 4096,
    "embedding": false,
    "prompt_template": "{system_message}\n### Instruction: {prompt}\n### Response:",
    "llama_model_path": "phi-2.Q4_K_S.gguf"
  },
  "parameters": {
    "temperature": 0.7,
    "top_p": 0.95,
    "stream": true,
    "max_tokens": 2048,
    "stop": ["<endofstring>"],
    "frequency_penalty": 0,
    "presence_penalty": 0
  },
  "metadata": {
    "size": 1615568736,
    "author": "User",
    "tags": []
  },
  "engine": "nitro"
}
```

:::warning

For Windows users, ensure to include double backslashes in the URL property, such as `C:\\Users\\username\\filename.gguf`.

:::

### Step 3: Start the Model

1. Restart Jan and go to the **Hub**, where the model will be automatically detected and displayed.
2. Find your model and click **Use** to try it out.