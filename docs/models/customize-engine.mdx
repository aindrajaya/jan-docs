---
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Customize Engine Settings

In this guide, we will show you how to customize the engine settings.

1. Navigate to the `~/jan/engine` folder. You can find this folder by going to `App Settings` > `Advanced` > `Open App Directory`.

<Tabs>
    <TabItem value="mac" label="MacOS" default>
        ```sh
        cd ~/jan/engines
        ```
    </TabItem>
    <TabItem value="windows" label="Windows" default>
        ```sh
        C:/Users/<your_user_name>/jan/engines
        ```
    </TabItem>
    <TabItem value="linux" label="Linux" default>
        ```sh
        cd ~/jan/engines
        ```
    </TabItem>
</Tabs>

2. Modify the `nitro.json` file based on your needs. The default settings are shown below.

```json title="~/jan/engines/nitro.json"
{
  "ctx_len": 2048,
  "ngl": 100,
  "cpu_threads": 1,
  "cont_batching": false,
  "embedding": false
}
```

The table below describes the parameters in the `nitro.json` file.

| Parameter | Type | Description |
| --------- | ---- | ----------- |
| `ctx_len` | **Integer** | Typically set at `2048`, `ctx_len` provides ample context for model operations like `GPT-3.5`. (*Maximum*: `4096`, *Minimum*: `1`) |
| `ngl` | **Integer** | Defaulted at `100`, `ngl` determines GPU layer usage. |
| `cpu_threads` | **Integer** | Determines CPU inference threads, limited by hardware and OS. (*Maximum* determined by system) |
| `cont_batching` | **Integer** | Controls continuous batching, enhancing throughput for LLM inference. |
| `embedding` | **Integer** | Enables embedding utilization for tasks like document-enhanced chat in RAG-based applications. |

### Additionl Info

- **`ctx_len`:**
  - Adjusting the value:
    - Maximizing widens the model's context, while minimizing focuses solely on the input prompt.
- **`ngl`:**
  - GPU utilization:
    - Set `ngl` to `15` to use 50% of the GPU, suitable for models like *Mistral* or *Llama* with around 30 layers.
    - At `ngl = 100`, all model layers load onto the GPU, typically containing 32-64 layers. Thus, behavior at `ngl = 400` mirrors that at `100`.
- **`cpu_threads`:**
  - Check system specifications and OS documentation to determine supported maximum thread count.
- **`cont_batching`:**
  - Explore the [Continuous Batching](https://nitro.jan.ai/features/cont-batch) section in the Nitro documentation.
  - Use cases:
    - Beneficial for scenarios like chatbots, consolidating multiple requests for improved performance.
    - Particularly effective for large language models, enhancing efficiency by merging requests into batches for faster responses.
    - Exercise caution as it may not suit all scenarios, such as brief model execution or highly variable requests. Compatibility may vary with models or inference backends.
- **`embedding`:**
  - Activation:
    - Enable embedding feature by adding the JSON parameter `"embedding" : true`.
    - Allows Nitro to process inferences with embedding capabilities. Refer to the Nitro documentation for [Embedding](https://nitro.jan.ai/features/embed) details.

:::info[Assistance and Support]

If you have questions, please join our [Discord community](https://discord.gg/Dt7MxDyNNZ) for support, updates, and discussions.

:::