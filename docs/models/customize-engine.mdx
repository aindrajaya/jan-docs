---
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Customize Engine Settings

In this guide, we will show you how to customize the engine settings.

1. Navigate to the `~/jan/engine` folder. You can find this folder by going to `App Settings` > `Advanced` > `Open App Directory`.

<Tabs>
    <TabItem value="mac" label="MacOS" default>
        ```sh
        cd ~/jan/engines
        ```
    </TabItem>
    <TabItem value="windows" label="Windows" default>
        ```sh
        C:/Users/<your_user_name>/jan/engines
        ```
    </TabItem>
    <TabItem value="linux" label="Linux" default>
        ```sh
        cd ~/jan/engines
        ```
    </TabItem>
</Tabs>

2. Modify the `nitro.json` file based on your needs. The default settings are shown below.

```json title="~/jan/engines/nitro.json"
{
  "ctx_len": 2048,
  "ngl": 100,
  "cpu_threads": 1,
  "cont_batching": false,
  "embedding": false
}
```

The table below describes the parameters in the `nitro.json` file.

| Parameter | Type | Description |
| --------- | ---- | ----------- |
| `ctx_len` | **Integer** | The context length for the model operations. |
| `ngl` | **Integer** | The number of GPU layers to use. |
| `cpu_threads` | **Integer** | The number of threads to use for inferencing (CPU mode only) |
| `cont_batching` | **Integer** | Whether to use continuous batching. |
| `embedding` | **Integer** | Whether to use embedding in the model. |

### Additionl Info

 - By default, `ngl` is set to `100`, offloading all.
    - To utilize 50% of the GPU, set `ngl` to `15`, as *Mistral* or *Llama* models typically have about 30 layers.
    - For more info, go to [this section](/docs/references/software).
 - To activate the embedding feature, add the JSON parameter `"embedding" : true`. 
    - This enables Nitro to process inferences with embedding capabilities. See the Nitro documentation for details on [Embedding](https://nitro.jan.ai/features/embed).
 - For improved throughput and reduced latency in large language model (LLM) inference, consult the [Continuous Batching](https://nitro.jan.ai/features/cont-batch) section in the Nitro documentation.
    - For more info regarding `cont_batching`, go to [this section](/docs/references/software)

:::info[Assistance and Support]

If you have questions, please join our [Discord community](https://discord.gg/Dt7MxDyNNZ) for support, updates, and discussions.

:::