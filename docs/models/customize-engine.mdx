---
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Customize Engine Settings

In this guide, we will show you how to customize the engine settings.

1. Navigate to the `App Settings` > `Advanced` > `Open App Directory` > `~/jan/engine` folder.

<Tabs>
    <TabItem value="mac" label="MacOS" default>
        ```sh
        cd ~/jan/engines
        ```
    </TabItem>
    <TabItem value="windows" label="Windows" default>
        ```sh
        C:/Users/<your_user_name>/jan/engines
        ```
    </TabItem>
    <TabItem value="linux" label="Linux" default>
        ```sh
        cd ~/jan/engines
        ```
    </TabItem>
</Tabs>

2. Modify the `nitro.json` file based on your needs. The default settings are shown below.

```json title="~/jan/engines/nitro.json"
{
  "ctx_len": 2048,
  "ngl": 100,
  "cpu_threads": 1,
  "cont_batching": false,
  "embedding": false
}
```

The table below describes the parameters in the `nitro.json` file.

| Parameter | Type | Description |
| --------- | ---- | ----------- |
| `ctx_len` | **Integer** | The context length for the model operations. |
| `ngl` | **Integer** | The number of GPU layers to use. |
| `cpu_threads` | **Integer** | The number of threads to use for inferencing (CPU mode only) |
| `cont_batching` | **Integer** | Whether to use continuous batching. |
| `embedding` | **Integer** | Whether to use embedding in the model. |

:::tip
 - By default, the value of `ngl` is set to 100, which indicates that it will offload all. If you wish to offload only 50% of the GPU, you can set `ngl` to 15 because most models on Mistral or Llama are around ~ 30 layers.
 - To utilize the embedding feature, include the JSON parameter `"embedding": true`. It will enable Nitro to process inferences with embedding capabilities. Please refer to the [Embedding in the Nitro documentation](https://nitro.jan.ai/features/embed) for a more detailed explanation.
 - To utilize the continuous batching feature to boost throughput and minimize latency in large language model (LLM) inference, please refer to the [Continuous Batching in the Nitro documentation](https://nitro.jan.ai/features/cont-batch).

:::

:::info[Assistance and Support]

If you have questions, please join our [Discord community](https://discord.gg/Dt7MxDyNNZ) for support, updates, and discussions.

:::